# ML-Engineering-Foundation ðŸš€

This repository documents my journey through the Stanford Machine Learning Specialization (DeepLearning.AI). As a Systems & Cybersecurity Engineer, I focus on understanding the underlying mechanics of AIâ€”from mathematical foundations to vectorized implementations.

## ðŸ§  Key Concepts Covered

### Week 1: Simple Linear Regression
- **Cost Function:** Implemented Mean Squared Error (MSE) to measure model accuracy.
- **Gradient Descent:** Applied batch gradient descent to minimize the cost function $J(w,b)$.
- **Math:** Verification of the convexity of the squared error cost function to ensure reaching the Global Minimum.

### Week 2: Multiple Linear Regression & Vectorization
- **Feature Scaling:** Implementation of Z-score normalization to accelerate gradient descent convergence.
- **Vectorization:** Optimized computations using NumPy `np.dot` to handle multiple features ($n$) efficiently, replacing slow Python loops.
- **Learning Rate Analysis:** Systematic debugging of gradient descent by monitoring cost $J$ over iterations.

### Week 3: Logistic Regression & Regularization
- **Sigmoid Function:** Implemented the $g(z)$ function to map linear outputs to probabilities $[0, 1]$.
- **Decision Boundary:** Established thresholds (0.5) to classify non-linear datasets (e.g., Microchip quality assurance).
- **Regularization (L2):** Added penalty terms to the cost function and gradient to prevent Overfitting, ensuring model robustness on unseen data.

## ðŸ’» Tech Stack
- **Language:** Python
- **Libraries:** NumPy, Matplotlib (Visualization)
- **Environment:** Jupyter Notebooks & Standalone Python Scripts

## ðŸ“ˆ Roadmap (Specialization Progress)
- [x] **Course 1: Supervised Machine Learning: Regression and Classification**
- [âž”] **Transition:** Moving to the full **Deep Learning Specialization** for advanced neural architectures.

---
*Completed the first certification of the Stanford Specialization. Moving towards Neural Networks next.*
